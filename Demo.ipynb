{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9861b5c-e1e5-4020-bc66-3c9aea063f15",
   "metadata": {},
   "source": [
    "# Generating 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90bc0ea8-479e-4125-a1b5-dd2f46823101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import torch\n",
    "import legacy\n",
    "import dnnlib\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import imageio\n",
    "import shutil\n",
    "import os\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63991578-0a08-4876-947b-9cb238ac0997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the trained model\n",
    "model_path = os.path.join(os.getcwd(), \"models\", \"oct.pkl\") # [\"NORMAL\", \"CNV\", \"DRUSEN\", \"DME\"]\n",
    "model_path = os.path.join(os.getcwd(), \"models\", \"oct_256_resized_1600.pkl\") # [\"DRUSEN\", \"NORMAL\", \"DME\", \"CNV\"]\n",
    "\n",
    "# Directory where temporary files should be stored\n",
    "directory = os.path.join(os.getcwd(), \"demo\")\n",
    "\n",
    "# Available classes to choose from\n",
    "classes = [\"DRUSEN\", \"NORMAL\", \"DME\", \"CNV\"]\n",
    "classes_dict = {\"DRUSEN\": 0, \"NORMAL\": 1, \"DME\": 2, \"CNV\": 3}\n",
    "\n",
    "# Torch device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d38dab-51e5-40c1-85c1-ba9c6cd597b6",
   "metadata": {},
   "source": [
    "# Generate an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453e8cf4-2738-412e-946c-23c90dd49c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate an image by choosing out of four classes \n",
    "def generate_noise_image(choice):\n",
    "    random_seed = np.random.randint(1, 9999)\n",
    "    execution = f\"python generate.py --outdir={directory} --seeds={random_seed} --network={model_path} --class={classes_dict[choice]} --vector-mode=True\"\n",
    "    os.system(execution)    \n",
    "    img_path = os.path.join(os.getcwd(), directory, f\"{classes_dict[choice]}_{str(random_seed).zfill(4)}.png\")\n",
    "    img = Image.open(img_path)\n",
    "    return img\n",
    "\n",
    "iface1 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=gr.inputs.Dropdown(choices=classes, default=\"NORMAL\", label=\"Class\"),\n",
    "    outputs=gr.outputs.Image(type=\"pil\").style(height=256, width=256)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3cc6c-e6b5-462a-820f-f53b92dea15b",
   "metadata": {},
   "source": [
    "# Interpolating between x images (z-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f379e6ad-e456-4d87-8257-45fef98bcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create a video of an interpolation between x images\n",
    "def generate_noise_image(sequence, frames, n_steps):\n",
    "    try:\n",
    "        shutil.rmtree(directory)\n",
    "    except OSError as error:\n",
    "        print(f\"Failed to delete directory '{directory}': {error}\")\n",
    "    \n",
    "    counts = [sequence.count(str(i)) for i in range(4)]\n",
    "    random_seeds = np.sort(np.random.randint(1, 9999, sum(counts)))\n",
    "    print(counts)\n",
    "    print(random_seeds)\n",
    "    lvecs = []\n",
    "    \n",
    "    for i in range(sum(counts)):\n",
    "        execution = f\"python generate.py --outdir={directory} --seeds={random_seeds[i]} --network={model_path} --class={sequence[i]} --vector-mode=True\"\n",
    "        os.system(execution)\n",
    "        print(\"Generated image!\")\n",
    "        \n",
    "        ################################################################################################################\n",
    "        class_idx = int(sequence[i])\n",
    "\n",
    "        with dnnlib.util.open_url(model_path) as fp:\n",
    "            G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def map_z_to_w(z, G):\n",
    "            label = torch.zeros((1, G.c_dim), device=device)\n",
    "            label[:, class_idx] = 1\n",
    "            w = G.mapping(z.to(device), label)\n",
    "            return w\n",
    "\n",
    "        # Load z from file.\n",
    "        z_path = os.path.join(os.getcwd(), \"demo\", f\"{sequence[i]}_{str(random_seeds[i]).zfill(4)}.npy\")\n",
    "        z_np = np.load(z_path)\n",
    "\n",
    "        # Convert `z_np` to a PyTorch tensor.\n",
    "        z = torch.as_tensor(z_np, device=device)\n",
    "\n",
    "        # Convert z to w.\n",
    "        w = map_z_to_w(z, G)\n",
    "        ################################################################################################################\n",
    "        \n",
    "        lvecs.append(w)\n",
    "\n",
    "    FPS = 60\n",
    "    FREEZE_STEPS = 30\n",
    "    STEPS = int(frames)\n",
    "    \n",
    "    with dnnlib.util.open_url(model_path) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)  # type: ignore\n",
    "    \n",
    "    video = imageio.get_writer(f'{directory}/video.mp4', mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
    "\n",
    "    for i in range(sum(counts) - 1):# load z_arr from npz file\n",
    "        diff = lvecs[i+1] - lvecs[i]\n",
    "        step = diff / STEPS\n",
    "        current = lvecs[i].clone()\n",
    "        target_uint8 = np.array([256, 256, 3], dtype=np.uint8)\n",
    "\n",
    "\n",
    "        for j in range(STEPS):\n",
    "            z = current.to(device)\n",
    "            synth_image = G.synthesis(z, noise_mode='const')\n",
    "            synth_image = (synth_image + 1) * (255 / 2)\n",
    "            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "            repeat = FREEZE_STEPS if j == 0 or j == (STEPS - 1) else 1\n",
    "\n",
    "            for i in range(repeat):\n",
    "                video.append_data(synth_image)\n",
    "            current = current + step\n",
    "\n",
    "    video.close()\n",
    "    \n",
    "    return f\"{directory}/video.mp4\"\n",
    "\n",
    "sequence = gr.inputs.Textbox(default=\"13\", label=\"Sequence input\")\n",
    "n_steps = gr.inputs.Number(default=100, label=\"Latent Steps\")\n",
    "frames = gr.inputs.Number(default=240, label=\"Frames\")\n",
    "output = gr.outputs.Video(type=\"mp4\").style(height=256, width=256)\n",
    "\n",
    "iface2 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=[sequence, frames, n_steps],\n",
    "    outputs=output,\n",
    "    description=\"This Gradio interface generates a video by morphing in the latent space between given class images. To use it, enter a string consisting only of letters from 0-3 to serve as class labels.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e465c-2a39-4f3e-8659-defb6de6bbc8",
   "metadata": {},
   "source": [
    "# Interpolating between x images (approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7870f356-c383-41fd-99c3-41175a36e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create a video of an interpolation between x images\n",
    "def generate_noise_image(sequence, frames, n_steps):\n",
    "    try:\n",
    "        shutil.rmtree(directory)\n",
    "    except OSError as error:\n",
    "        print(f\"Failed to delete directory '{directory}': {error}\")\n",
    "    \n",
    "    counts = [sequence.count(str(i)) for i in range(4)]\n",
    "    random_seeds = np.sort(np.random.randint(1, 9999, sum(counts)))\n",
    "    print(counts)\n",
    "    print(random_seeds)\n",
    "    lvecs = []\n",
    "    \n",
    "    for i in range(sum(counts)):\n",
    "        execution = f\"python generate.py --outdir={directory} --seeds={random_seeds[i]} --network={model_path} --class={sequence[i]}\"\n",
    "        os.system(execution)\n",
    "        \n",
    "        outdir_path = os.path.join(directory, \"projections\", str(i).zfill(2))\n",
    "        target_dir = os.path.join(directory, f\"{sequence[i]}_{str(random_seeds[i]).zfill(4)}.png\")\n",
    "        execution = f\"python projector.py --outdir={outdir_path} --target={target_dir} --network={model_path} --num-steps={int(n_steps)} --save-video=True\"\n",
    "        os.system(execution)\n",
    "        \n",
    "        lvec_path = os.path.join(os.getcwd(), directory, \"projections\", f\"{str(i).zfill(2)}\", \"projected_w.npz\") \n",
    "        lvecs.append(np.load(lvec_path)['w'])\n",
    "\n",
    "    FPS = 60\n",
    "    FREEZE_STEPS = 30\n",
    "    STEPS = int(frames)\n",
    "    \n",
    "    with dnnlib.util.open_url(model_path) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)  # type: ignore\n",
    "    \n",
    "    video_path = os.path.join(os.getcwd(), directory, \"video.mp4\")\n",
    "    video = imageio.get_writer(video_path, mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
    "\n",
    "    for i in range(sum(counts) - 1):\n",
    "        diff = lvecs[i+1] - lvecs[i]\n",
    "        step = diff / STEPS\n",
    "        current = lvecs[i].copy()\n",
    "        target_uint8 = np.array([256, 256, 3], dtype=np.uint8)\n",
    "\n",
    "\n",
    "        for j in range(STEPS):\n",
    "            z = torch.from_numpy(current).to(device)\n",
    "            synth_image = G.synthesis(z, noise_mode='const')\n",
    "            synth_image = (synth_image + 1) * (255 / 2)\n",
    "            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "            repeat = FREEZE_STEPS if j == 0 or j == (STEPS - 1) else 1\n",
    "\n",
    "            for i in range(repeat):\n",
    "                video.append_data(synth_image)\n",
    "            current = current + step\n",
    "\n",
    "    video.close()\n",
    "    \n",
    "    return os.path.join(os.getcwd(), directory, \"video.mp4\")\n",
    "\n",
    "sequence = gr.inputs.Textbox(default=\"13\", label=\"Sequence input\")\n",
    "n_steps = gr.inputs.Number(default=100, label=\"Latent Steps\")\n",
    "frames = gr.inputs.Number(default=240, label=\"Frames\")\n",
    "output = gr.outputs.Video(type=\"mp4\").style(height=256, width=256)\n",
    "\n",
    "iface3 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=[sequence, frames, n_steps],\n",
    "    outputs=output,\n",
    "    description=\"This Gradio interface generates a video by morphing in the latent space between given class images. To use it, enter a string consisting only of letters from 0-3 to serve as class labels.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3201e-4790-46f8-9c0d-fe5a706001d5",
   "metadata": {},
   "source": [
    "# Generate variations of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a12b09d-b9c3-4a82-9543-8e2debd60f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create a video of an interpolation between x images\n",
    "def generate_noise_image(sequence, frames):\n",
    "    try:\n",
    "        shutil.rmtree(directory)\n",
    "    except OSError as error:\n",
    "        print(f\"Failed to delete directory '{directory}': {error}\")\n",
    "    \n",
    "    counts = [sequence.count(str(i)) for i in range(4)]\n",
    "    random_seeds = [np.random.randint(1, 9999)] * sum(counts)\n",
    "    print(counts)\n",
    "    print(random_seeds)\n",
    "    lvecs = []\n",
    "    \n",
    "    for i in range(sum(counts)):\n",
    "        execution = f\"python generate.py --outdir={directory} --seeds={random_seeds[i]} --network={model_path} --class={sequence[i]} --vector-mode=True\"\n",
    "        os.system(execution)\n",
    "        print(\"Generated image!\")\n",
    "        \n",
    "        ################################################################################################################\n",
    "        class_idx = int(sequence[i])\n",
    "\n",
    "        with dnnlib.util.open_url(model_path) as fp:\n",
    "            G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def map_z_to_w(z, G):\n",
    "            label = torch.zeros((1, G.c_dim), device=device)\n",
    "            label[:, class_idx] = 1\n",
    "            w = G.mapping(z.to(device), label)\n",
    "            return w\n",
    "\n",
    "        # Load z from file.\n",
    "        # z_path = os.path.join(os.getcwd(), \"demo\", f\"seed{str(random_seeds[i]).zfill(4)}.npy\")\n",
    "        z_path = os.path.join(os.getcwd(), \"demo\", f\"{sequence[i]}_{str(random_seeds[i]).zfill(4)}.npy\")\n",
    "        z_np = np.load(z_path)\n",
    "\n",
    "        # Convert `z_np` to a PyTorch tensor.\n",
    "        z = torch.as_tensor(z_np, device=device)\n",
    "\n",
    "        # Convert z to w.\n",
    "        w = map_z_to_w(z, G)\n",
    "        ################################################################################################################\n",
    "        \n",
    "        lvecs.append(w)\n",
    "\n",
    "    FPS = 60\n",
    "    FREEZE_STEPS = 30\n",
    "    STEPS = int(frames)\n",
    "    \n",
    "    with dnnlib.util.open_url(model_path) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)  # type: ignore\n",
    "    \n",
    "    video = imageio.get_writer(f'{directory}/video.mp4', mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
    "\n",
    "    for i in range(sum(counts) - 1):# load z_arr from npz file\n",
    "        diff = lvecs[i+1] - lvecs[i]\n",
    "        step = diff / STEPS\n",
    "        current = lvecs[i].clone()\n",
    "        target_uint8 = np.array([256, 256, 3], dtype=np.uint8)\n",
    "\n",
    "\n",
    "        for j in range(STEPS):\n",
    "            z = current.to(device)\n",
    "            synth_image = G.synthesis(z, noise_mode='const')\n",
    "            synth_image = (synth_image + 1) * (255 / 2)\n",
    "            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "            repeat = FREEZE_STEPS if j == 0 or j == (STEPS - 1) else 1\n",
    "\n",
    "            for i in range(repeat):\n",
    "                video.append_data(synth_image)\n",
    "            current = current + step\n",
    "\n",
    "    video.close()\n",
    "    \n",
    "    return f\"{directory}/video.mp4\"\n",
    "\n",
    "sequence = gr.inputs.Textbox(default=\"13\", label=\"Sequence input\")\n",
    "frames = gr.inputs.Number(default=240, label=\"Frames\")\n",
    "output = gr.outputs.Video(type=\"mp4\").style(height=256, width=256)\n",
    "\n",
    "iface4 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=[sequence, frames],\n",
    "    outputs=output,\n",
    "    description=\"This Gradio interface generates a video by morphing in the latent space between given class images. To use it, enter a string consisting only of letters from 0-3 to serve as class labels.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a58259-0dc4-4ae6-adcb-9805ae217961",
   "metadata": {},
   "source": [
    "# Multiple Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03643f0a-a044-4d43-bfd2-9f79b46fe52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://067b0bbb-ac19-4f48.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://067b0bbb-ac19-4f48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1]\n",
      "[1007 3006]\n",
      "Generated image!\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... [0, 1, 0, 1]\n",
      "[6079 7666]\n",
      "Done.\n",
      "Generated image!\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "[0, 1, 0, 1]\n",
      "[1431, 1431]\n",
      "Generated image!\n",
      "Generated image!\n"
     ]
    }
   ],
   "source": [
    "gr.TabbedInterface(\n",
    "    [iface1, iface2, iface3, iface4], [\"Generate an image\", \"Generate a video (original)\", \"Generate a video (approximated)\", \"Generate variations of an image\"]\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e17ec-79ba-40c7-9c64-74dbd80e1fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
