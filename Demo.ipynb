{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9861b5c-e1e5-4020-bc66-3c9aea063f15",
   "metadata": {},
   "source": [
    "# Generating 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90bc0ea8-479e-4125-a1b5-dd2f46823101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import torch\n",
    "import legacy\n",
    "import dnnlib\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import imageio\n",
    "import shutil\n",
    "import os\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63991578-0a08-4876-947b-9cb238ac0997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the trained model\n",
    "model_path = os.path.join(os.getcwd(), \"models\", \"oct.pkl\") # [\"NORMAL\", \"CNV\", \"DRUSEN\", \"DME\"]\n",
    "model_path = os.path.join(os.getcwd(), \"models\", \"oct_256_resized_1600.pkl\") # [\"DRUSEN\", \"NORMAL\", \"DME\", \"CNV\"]\n",
    "\n",
    "# Directory where temporary files should be stored\n",
    "directory = os.path.join(os.getcwd(), \"demo\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory):\n",
    "    # If it exists, delete it and its contents\n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(directory)\n",
    "\n",
    "# Available classes to choose from\n",
    "classes = [\"DRUSEN\", \"NORMAL\", \"DME\", \"CNV\"]\n",
    "classes_dict = {\"DRUSEN\": 0, \"NORMAL\": 1, \"DME\": 2, \"CNV\": 3}\n",
    "\n",
    "# Torch device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d38dab-51e5-40c1-85c1-ba9c6cd597b6",
   "metadata": {},
   "source": [
    "# Generate an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453e8cf4-2738-412e-946c-23c90dd49c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate an image by choosing out of four classes \n",
    "def generate_noise_image(choice):\n",
    "    random_seed = np.random.randint(1, 9999)\n",
    "    execution = f\"python generate.py --outdir={directory} --seeds={random_seed} --network={model_path} --class={classes_dict[choice]} --vector-mode=True\"\n",
    "    os.system(execution)    \n",
    "    img_path = os.path.join(os.getcwd(), directory, f\"{classes_dict[choice]}_{str(random_seed).zfill(4)}.png\")\n",
    "    img = Image.open(img_path)\n",
    "    return img\n",
    "\n",
    "iface1 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=gr.inputs.Dropdown(choices=classes, default=\"NORMAL\", label=\"Class\"),\n",
    "    outputs=gr.outputs.Image(type=\"pil\").style(height=256, width=256)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3cc6c-e6b5-462a-820f-f53b92dea15b",
   "metadata": {},
   "source": [
    "# Interpolating between x images (z-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59cc0bbf-f19d-4b9d-a8bf-54ff9508ff9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_images(image1_path, image2_path, output_path):\n",
    "    # Open the images\n",
    "    image1 = Image.open(image1_path)\n",
    "    image2 = Image.open(image2_path)\n",
    "\n",
    "    # Determine the maximum width and total height\n",
    "    max_width = max(image1.width, image2.width)\n",
    "    total_height = max(image1.height, image2.height)\n",
    "\n",
    "    # Create a new blank image with the combined size\n",
    "    combined_image = Image.new(\"RGB\", (max_width * 2, total_height))\n",
    "\n",
    "    # Paste the first image on the left side\n",
    "    combined_image.paste(image1, (0, 0))\n",
    "\n",
    "    # Paste the second image on the right side\n",
    "    combined_image.paste(image2, (max_width, 0))\n",
    "\n",
    "    # Save the combined image\n",
    "    combined_image.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f379e6ad-e456-4d87-8257-45fef98bcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create a video of an interpolation between x images\n",
    "def generate_noise_image(sequence, frames):\n",
    "    try:\n",
    "        shutil.rmtree(directory)\n",
    "    except OSError as error:\n",
    "        print(f\"Failed to delete directory '{directory}': {error}\")\n",
    "    \n",
    "    counts = [sequence.count(str(i)) for i in range(4)]\n",
    "    random_seeds = np.sort(np.random.randint(1, 9999, sum(counts)))\n",
    "    lvecs = []\n",
    "    generated_images = []  # Store the generated images\n",
    "    \n",
    "    for i in range(sum(counts)):\n",
    "        execution = f\"python generate.py --outdir={directory} --seeds={random_seeds[i]} --network={model_path} --class={sequence[i]} --vector-mode=True\"\n",
    "        os.system(execution)\n",
    "        \n",
    "        generated_image_path = os.path.join(directory, f\"{sequence[i]}_{str(random_seeds[i]).zfill(4)}.png\")\n",
    "        \n",
    "        # Read the image using PIL\n",
    "        # image = Image.open(generated_image_path)\n",
    "        \n",
    "        # Convert the image to a NumPy array\n",
    "        # image_array = np.array(image)\n",
    "        \n",
    "        generated_images.append(generated_image_path)\n",
    "        \n",
    "        ################################################################################################################\n",
    "        class_idx = int(sequence[i])\n",
    "\n",
    "        with dnnlib.util.open_url(model_path) as fp:\n",
    "            G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def map_z_to_w(z, G):\n",
    "            label = torch.zeros((1, G.c_dim), device=device)\n",
    "            label[:, class_idx] = 1\n",
    "            w = G.mapping(z.to(device), label)\n",
    "            return w\n",
    "\n",
    "        # Load z from file.\n",
    "        z_path = os.path.join(os.getcwd(), \"demo\", f\"{sequence[i]}_{str(random_seeds[i]).zfill(4)}.npy\")\n",
    "        z_np = np.load(z_path)\n",
    "\n",
    "        # Convert `z_np` to a PyTorch tensor.\n",
    "        z = torch.as_tensor(z_np, device=device)\n",
    "\n",
    "        # Convert z to w.\n",
    "        w = map_z_to_w(z, G)\n",
    "        ################################################################################################################\n",
    "        \n",
    "        lvecs.append(w)\n",
    "\n",
    "    FPS = 60\n",
    "    FREEZE_STEPS = 30\n",
    "    STEPS = int(frames)\n",
    "    \n",
    "    with dnnlib.util.open_url(model_path) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)  # type: ignore\n",
    "    \n",
    "    video = imageio.get_writer(f'{directory}/video.mp4', mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
    "\n",
    "    for i in range(sum(counts) - 1):# load z_arr from npz file\n",
    "        diff = lvecs[i+1] - lvecs[i]\n",
    "        step = diff / STEPS\n",
    "        current = lvecs[i].clone()\n",
    "        target_uint8 = np.array([256, 256, 3], dtype=np.uint8)\n",
    "\n",
    "\n",
    "        for j in range(STEPS):\n",
    "            z = current.to(device)\n",
    "            synth_image = G.synthesis(z, noise_mode='const')\n",
    "            synth_image = (synth_image + 1) * (255 / 2)\n",
    "            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "            repeat = FREEZE_STEPS if j == 0 or j == (STEPS - 1) else 1\n",
    "\n",
    "            for i in range(repeat):\n",
    "                video.append_data(synth_image)\n",
    "            current = current + step\n",
    "\n",
    "    video.close()\n",
    "    \n",
    "    out_img = os.path.join(directory, 'combined.png')\n",
    "    combine_images(generated_images[0], generated_images[1], out_img)\n",
    "    \n",
    "    vid = os.path.join(directory, 'video.mp4')\n",
    "    img = os.path.join(directory, 'combined.png')\n",
    "    \n",
    "    return vid, img #generated_images[0], generated_images[1]\n",
    "\n",
    "sequence = gr.inputs.Textbox(default=\"13\", label=\"Sequence input\")\n",
    "frames = gr.inputs.Number(default=240, label=\"Frames\")\n",
    "output_video = gr.outputs.Video(type=\"mp4\").style(height=256, width=256)\n",
    "output_image = gr.outputs.Image(type=\"filepath\", label=\"Generated Images\").style(height=256, width=512)\n",
    "# output_image1 = gr.outputs.Image(type=\"numpy\", label=\"Generated Image 1\")\n",
    "# output_image1 = gr.outputs.Image(type=\"filepath\", label=\"Generated Image 1\").style(height=256, width=256)\n",
    "# output_image2 = gr.outputs.Image(type=\"filepath\", label=\"Generated Image 2\").style(height=256, width=256)\n",
    "\n",
    "# output = gr.outputs.Video(type=\"numpy\").style(height=256, width=256)\n",
    "\n",
    "iface2 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=[sequence, frames],\n",
    "    outputs=[output_video, output_image],\n",
    "    description=\"This Gradio interface generates a video by morphing in the latent space between given class images. To use it, enter a string consisting only of letters from 0-3 to serve as class labels.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e465c-2a39-4f3e-8659-defb6de6bbc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interpolating between x images (approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7870f356-c383-41fd-99c3-41175a36e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create a video of an interpolation between x images\n",
    "def generate_noise_image(sequence, frames, n_steps):\n",
    "    try:\n",
    "        shutil.rmtree(directory)\n",
    "    except OSError as error:\n",
    "        print(f\"Failed to delete directory '{directory}': {error}\")\n",
    "    \n",
    "    counts = [sequence.count(str(i)) for i in range(4)]\n",
    "    random_seeds = np.sort(np.random.randint(1, 9999, sum(counts)))\n",
    "    lvecs = []\n",
    "    \n",
    "    for i in range(sum(counts)):\n",
    "        execution = f\"python generate.py --outdir={directory} --seeds={random_seeds[i]} --network={model_path} --class={sequence[i]}\"\n",
    "        os.system(execution)\n",
    "        \n",
    "        outdir_path = os.path.join(directory, \"projections\", str(i).zfill(2))\n",
    "        target_dir = os.path.join(directory, f\"{sequence[i]}_{str(random_seeds[i]).zfill(4)}.png\")\n",
    "        execution = f\"python projector.py --outdir={outdir_path} --target={target_dir} --network={model_path} --num-steps={int(n_steps)} --save-video=True\"\n",
    "        os.system(execution)\n",
    "        \n",
    "        lvec_path = os.path.join(os.getcwd(), directory, \"projections\", f\"{str(i).zfill(2)}\", \"projected_w.npz\") \n",
    "        lvecs.append(np.load(lvec_path)['w'])\n",
    "\n",
    "    FPS = 60\n",
    "    FREEZE_STEPS = 30\n",
    "    STEPS = int(frames)\n",
    "    \n",
    "    with dnnlib.util.open_url(model_path) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)  # type: ignore\n",
    "    \n",
    "    video_path = os.path.join(os.getcwd(), directory, \"video.mp4\")\n",
    "    video = imageio.get_writer(video_path, mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
    "\n",
    "    for i in range(sum(counts) - 1):\n",
    "        diff = lvecs[i+1] - lvecs[i]\n",
    "        step = diff / STEPS\n",
    "        current = lvecs[i].copy()\n",
    "        target_uint8 = np.array([256, 256, 3], dtype=np.uint8)\n",
    "\n",
    "\n",
    "        for j in range(STEPS):\n",
    "            z = torch.from_numpy(current).to(device)\n",
    "            synth_image = G.synthesis(z, noise_mode='const')\n",
    "            synth_image = (synth_image + 1) * (255 / 2)\n",
    "            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "            repeat = FREEZE_STEPS if j == 0 or j == (STEPS - 1) else 1\n",
    "\n",
    "            for i in range(repeat):\n",
    "                video.append_data(synth_image)\n",
    "            current = current + step\n",
    "\n",
    "    video.close()\n",
    "    \n",
    "    return os.path.join(os.getcwd(), directory, \"video.mp4\")\n",
    "\n",
    "sequence = gr.inputs.Textbox(default=\"13\", label=\"Sequence input\")\n",
    "n_steps = gr.inputs.Number(default=100, label=\"Latent Steps\")\n",
    "frames = gr.inputs.Number(default=240, label=\"Frames\")\n",
    "output = gr.outputs.Video(type=\"mp4\").style(height=256, width=256)\n",
    "\n",
    "iface3 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=[sequence, frames, n_steps],\n",
    "    outputs=output,\n",
    "    description=\"This Gradio interface generates a video by morphing in the latent space between given class images. To use it, enter a string consisting only of letters from 0-3 to serve as class labels.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3201e-4790-46f8-9c0d-fe5a706001d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate variations of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a12b09d-b9c3-4a82-9543-8e2debd60f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create a video of an interpolation between x images\n",
    "def generate_noise_image(sequence, frames):\n",
    "    try:\n",
    "        shutil.rmtree(directory)\n",
    "    except OSError as error:\n",
    "        print(f\"Failed to delete directory '{directory}': {error}\")\n",
    "    \n",
    "    counts = [sequence.count(str(i)) for i in range(4)]\n",
    "    random_seeds = [np.random.randint(1, 9999)] * sum(counts)\n",
    "    lvecs = []\n",
    "    \n",
    "    for i in range(sum(counts)):\n",
    "        execution = f\"python generate.py --outdir={directory} --seeds={random_seeds[i]} --network={model_path} --class={sequence[i]} --vector-mode=True\"\n",
    "        # os.system(execution)\n",
    "        subprocess.run(execution, shell=True)\n",
    "        \n",
    "        ################################################################################################################\n",
    "        class_idx = int(sequence[i])\n",
    "\n",
    "        with dnnlib.util.open_url(model_path) as fp:\n",
    "            G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def map_z_to_w(z, G):\n",
    "            label = torch.zeros((1, G.c_dim), device=device)\n",
    "            label[:, class_idx] = 1\n",
    "            w = G.mapping(z.to(device), label)\n",
    "            return w\n",
    "\n",
    "        # Load z from file.\n",
    "        # z_path = os.path.join(os.getcwd(), \"demo\", f\"seed{str(random_seeds[i]).zfill(4)}.npy\")\n",
    "        z_path = os.path.join(os.getcwd(), \"demo\", f\"{sequence[i]}_{str(random_seeds[i]).zfill(4)}.npy\")\n",
    "        z_np = np.load(z_path)\n",
    "\n",
    "        # Convert `z_np` to a PyTorch tensor.\n",
    "        z = torch.as_tensor(z_np, device=device)\n",
    "\n",
    "        # Convert z to w.\n",
    "        w = map_z_to_w(z, G)\n",
    "        ################################################################################################################\n",
    "        \n",
    "        lvecs.append(w)\n",
    "\n",
    "    FPS = 60\n",
    "    FREEZE_STEPS = 30\n",
    "    STEPS = int(frames)\n",
    "    \n",
    "    with dnnlib.util.open_url(model_path) as fp:\n",
    "        G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)  # type: ignore\n",
    "    \n",
    "    video = imageio.get_writer(f'{directory}/video.mp4', mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
    "\n",
    "    for i in range(sum(counts) - 1):# load z_arr from npz file\n",
    "        diff = lvecs[i+1] - lvecs[i]\n",
    "        step = diff / STEPS\n",
    "        current = lvecs[i].clone()\n",
    "        target_uint8 = np.array([256, 256, 3], dtype=np.uint8)\n",
    "\n",
    "\n",
    "        for j in range(STEPS):\n",
    "            z = current.to(device)\n",
    "            synth_image = G.synthesis(z, noise_mode='const')\n",
    "            synth_image = (synth_image + 1) * (255 / 2)\n",
    "            synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "            repeat = FREEZE_STEPS if j == 0 or j == (STEPS - 1) else 1\n",
    "\n",
    "            for i in range(repeat):\n",
    "                video.append_data(synth_image)\n",
    "            current = current + step\n",
    "\n",
    "    video.close()\n",
    "    \n",
    "    return f\"{directory}/video.mp4\"\n",
    "\n",
    "sequence = gr.inputs.Textbox(default=\"13\", label=\"Sequence input\")\n",
    "frames = gr.inputs.Number(default=240, label=\"Frames\")\n",
    "output = gr.outputs.Video(type=\"mp4\").style(height=256, width=256)\n",
    "\n",
    "iface4 = gr.Interface(\n",
    "    fn=generate_noise_image,\n",
    "    inputs=[sequence, frames],\n",
    "    outputs=output,\n",
    "    description=\"This Gradio interface generates a video by morphing in the latent space between given class images. To use it, enter a string consisting only of letters from 0-3 to serve as class labels.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15caec-e651-4b57-a0e5-22a03ac5b0a1",
   "metadata": {},
   "source": [
    "# Approximate a given image in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af60d5f-6762-4452-8d75-256e3a5f000d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def edit_image(input_image, num_steps=1000):\n",
    "    pil_image = Image.fromarray(input_image)\n",
    "    pil_image.save(os.path.join(directory, 'input_image.png'))\n",
    "    execution = f\"python projector.py --outdir={directory} --network={model_path} --target={os.path.join(directory, 'input_image.png')} --num-steps={int(num_steps)}\"\n",
    "    os.system(execution)\n",
    "    \n",
    "    # return execution #os.path.join(directory, 'proj.png')\n",
    "    return os.path.join(directory, 'proj.png'), os.path.join(directory, 'proj.mp4')\n",
    "\n",
    "steps = gr.inputs.Number(default=1000, label=\"Steps\")\n",
    "input_image = gr.outputs.Image(type=\"numpy\").style(height=256, width=256)\n",
    "output_image = gr.outputs.Image(type=\"pil\").style(height=256, width=256)\n",
    "output_video = gr.outputs.Video(type=\"mp4\").style(height=256, width=512)\n",
    "\n",
    "iface5 = gr.Interface(\n",
    "    fn=edit_image, \n",
    "    inputs=[input_image, steps], \n",
    "    input_names=[\"Input Image\", \"Number of Steps\"],\n",
    "    input_labels=[\"Choose an image\", \"Number of Steps\"],\n",
    "    \n",
    "    outputs=[output_image, output_video],\n",
    "    output_names=[\"Edited Image\", \"Video File\"],\n",
    "    output_labels=[\"Edited Image\", \"Video File\"],\n",
    "    input_is_default=[True, False],\n",
    "    description=\"This Gradio interface generates a latent approximation using an input image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a58259-0dc4-4ae6-adcb-9805ae217961",
   "metadata": {},
   "source": [
    "# Multiple Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03643f0a-a044-4d43-bfd2-9f79b46fe52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://a57e0964-3cbe-4ca5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a57e0964-3cbe-4ca5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.TabbedInterface(\n",
    "    [iface1, iface2, iface3, iface4, iface5], [\"Generate an image\", \"Morph two images (latent)\", \"Morph two images (projection)\", \"Generate variations of an image\", \"Project an image\"]\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4b3d1-b38c-45df-a4dc-5e064f5194e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
